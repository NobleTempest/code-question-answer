{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length = 256\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    \"\"\"A single training/test example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 idx,\n",
    "                 source_question,\n",
    "                 source_code,\n",
    "                 target,\n",
    "                 ):\n",
    "        self.idx = idx\n",
    "        self.source_question = source_question\n",
    "        self.source_code = source_code\n",
    "        self.target = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename, stage):\n",
    "    \"\"\"Read examples from filename.\"\"\"\n",
    "    # filename: e.g. $data_dir/$lang/train/\n",
    "    # stage: e.g. train\n",
    "    examples=[]\n",
    "    idx = 0\n",
    "    codefile = os.path.join(filename, stage + \".code\")\n",
    "    quesfile = os.path.join(filename, stage + \".question\")\n",
    "    ansfile = os.path.join(filename, stage + \".answer\")\n",
    "    with open(codefile,encoding=\"utf-8\") as code_f:\n",
    "        with open(ansfile, encoding=\"utf-8\") as ans_f:\n",
    "            with open(quesfile, encoding=\"utf-8\") as ques_f:\n",
    "                for codeline, quesline, ansline in zip(code_f,ques_f,ans_f):\n",
    "                    code = codeline.strip()\n",
    "                    question = quesline.strip()\n",
    "                    ans = ansline.strip()\n",
    "                    examples.append(\n",
    "                        Example(\n",
    "                                idx = idx,\n",
    "                                # source= question + \" \" + code,\n",
    "                                source_question = question,\n",
    "                                source_code = code,\n",
    "                                target = ans,\n",
    "                                )\n",
    "                    )\n",
    "                    idx += 1\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 source_ids,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer,stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(examples):\n",
    "        #source\n",
    "        source_question_tokens = tokenizer.tokenize(example.source_question)\n",
    "        source_code_tokens = tokenizer.tokenize(example.source_code)\n",
    "        source_tokens = source_question_tokens + [tokenizer.sep_token] + source_code_tokens\n",
    "        source_tokens = source_tokens[:max_source_length-2]\n",
    "        source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
    "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        padding_length = max_source_length - len(source_ids)\n",
    "        source_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_mask+=[0]*padding_length\n",
    " \n",
    "        #target\n",
    "        if stage==\"test\":\n",
    "            target_tokens = tokenizer.tokenize(\"None\")\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[:max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] *len(target_ids)\n",
    "        padding_length = max_target_length - len(target_ids)\n",
    "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        target_mask+=[0]*padding_length   \n",
    "   \n",
    "        if example_index < 5:\n",
    "            if stage=='train':\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"idx: {}\".format(example.idx))\n",
    "\n",
    "                logger.info(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
    "                logger.info(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
    "                logger.info(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
    "                \n",
    "                logger.info(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
    "                logger.info(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
    "                logger.info(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
    "       \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 source_ids,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "            )\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = read_examples('D:/NLP/codeQA/train', 'train')\n",
    "train_features = convert_examples_to_features(train_examples, tokenizer,stage='train')\n",
    "all_source_ids = torch.tensor([f.source_ids for f in train_features], dtype=torch.long)\n",
    "all_source_mask = torch.tensor([f.source_mask for f in train_features], dtype=torch.long)\n",
    "all_target_ids = torch.tensor([f.target_ids for f in train_features], dtype=torch.long)\n",
    "all_target_mask = torch.tensor([f.target_mask for f in train_features], dtype=torch.long)    \n",
    "train_data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbc5014ed32a3511c99c810aecedbd8e5a5c878610fa20b4667810c497ccaeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
